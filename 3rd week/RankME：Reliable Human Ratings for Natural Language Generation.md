## RankME：Reliable Human Ratings for Natural Language Generation
### 摘要
- 评价者之间存在不一致的问题，之前将此问题归因于评价者个人偏好，但本文证明人工评价的质量可以通过实验设计提高
- 提出基于等级的幅度估计方法（rank-based magnitude estimation method (RankME)），结合使用了连续量表（continuous scales）和相对评估（relative assessments）
- RankME极大地提高了评价者地可靠性和一致性

### 简介
- 人工评价是自然语言生成主要的评价方式，但是有很少有研究者去研究如何提高人工评价的可靠性
- 不同的评价标准：NLG根据多个不同的标准进行评价如自然性和信息性
    - 自然性：流畅性和可读性，旨在评价文本的语法
    - 信息性：充分性或正确性，旨在评价生成文本的语义内容的正确性与和输入文本的相关性
- 评价者无法准确评价这些不同的方面，因而只评价一个总体的质量标准
- 使用独特的任务设计，仍然可以获得不同方面的判别性评分
- 评价者的判断存在不一致的情况，对于同一个生成文本，判断结果差异较大，可通过改变实验设置提高
- 相对 vs 绝对评价，相对评估方法比直接评估方法产生更一致，更具判别性的人类评分
- RankME方法要优于传统人工评价方法：连续量表，幅度估计，相对评价

### 实验设置
- 对三个生成模型进行评价 Sheffiel  Slug2Slug TGen
- 使用三种方式收集人工评价的结果：6分李克特量表，量级估计，和基于等级的量级估计
- 量级估计ME：给定参考回复和一个固定的分数，评价者将生成回复与参考回复比较，
若表现优于参考回复两倍，那么生成回复的得分为固定分数的两倍，若表现只有参考回复的一半好，那么得分为固定得分的一半。其使用的是无数字标签的连续量表
- RankME对ME进行了扩展，要求评价者提供生成文本的相对排名

### 对多个评价指标进行评价
- 信息性/充分性：话语是否根据输入语句提供了有用信息？
- 自然性/流畅性：话语可能是由本土人说的吗？
- 质量：您如何从语法正确性，流利度，适当性和其他重要因素方面判断话语的整体质量？
- 使用前面提到的三种实验设置分别对这三个评价指标同时判断和独立判断

### 评价者之间的一致性 直接 vs 相对比较
- 连续的量表可以让评价者做出更细微的判断，因而比李克特量表效果要好
- 两两比较 binary comparisons
- RanME具有很大的优势，但该方法比较次数与要比较的系统数量成正比增长。
- 对于较大数量的系统，如公开比赛中，采用数据有效的排名算法 TrueSkill Data-efficient ranking algorithms
- 为自动评分模型的训练提供更加可靠的人工评分
