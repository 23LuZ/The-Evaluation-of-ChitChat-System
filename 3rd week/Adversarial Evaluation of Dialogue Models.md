## Adversarial Evaluation of Dialogue Models

### 摘要 Abstract
- 对抗性损失 Adversarial loss可以是一种直接评价生成回复有多大程度像人类回复的方式
- 训练一个RNN模型来区分对话模型生成的回复和人类产生的回复
- 尽管实验证明这种方式可行，但在实际应用中仍存在许多问题，需要进一步研究

### 简介 Introduction
- 人工评价不能规模化，而且在一些敏感情况下一些数据不能被人看到，人工评价也是有问题的
- 虽然困惑度是衡量模型拟合某些数据的好方法，但它不能衡量特定任务的性能
- 假设一个模型质量的好坏跟它的回复有多容易与人类回复区分来有关
- 将一个经过训练的对话模型部署为‘生成器’并保持不变，再训练一个RNN‘判别器’：给定问题和回复，判断回复是生成器生成的还是从人类回复中采样得到的
- 目的是为了了解对抗性设置是否能够用于评价
- 实验结果证明，判别器能够在超过60%的情况下将模型生成的回复和人类产生的回复区分开
- 判别器的结果依赖于长度信息，和简单通用回复，从侧面也说明了生成器的缺点
- **缺乏证据表明使判别器准确率较低的模型在人类评价中会更好**

### 模型 Model
- 像GAN，模型包含一个生成器和一个判别器，两个分开进行训练
- 生成器是一个seq2seq模型，判别器是一个二分类模型

### 实验 Experiments
- 数据：Email reply 
- 判别器使用与生成器不同的训练邮件回复数据进行训练，一半保持回复不变并标记为1，另一半的回复为生成器的回复并标记为0
- 判别器能够在62.5%的情况下区分模型和人类的回复

### 与PPL进行比较 Comparsion with perplexity
- 定量结果显示判别器目标与生成目标具有不同的特征
- 判别器的喜好与时长密切相关，判别器会给长句子打高分甚至句子是不连贯的，可能与seq2seq模型具有长度偏差有关
- 对于相同长度的句子，判别器也没给予相同的概率
- 当比较相同长度的回复时，判别器的概率分数与生成器的似然概率分数明显不同，平均Spearman相关系数为-0.02
- 判别器不喜欢生成器生成的通用回复‘thank you’，‘yes’，可以说检测出了生成器的缺点
- 与长度一样，判别器偏爱稀有语言 rare language 不一定意味着更好的回复
- 未来的工作可能会应用小批量判别 minibatch discrimination，以更明确的解决多样性的问题

### 讨论 Discussion
- 训练一个判别器是很简单的，任何研究小组都可以训练一个判别器然后对模型进行评价
- 判别器能够识别出生成器的一些弱点：长度问题和多样性
- 与使用beam search 相比，判别器在采样时费时费力，但这与人类的观察结果相矛盾，后者通常需要进行一些搜索才能获得最高质量的回复
- 需要进一步的工作确认判别器是否能够应用到对话评价领域
