## Adversarial Evaluation for Open-Domain Dialogue Generation
### 摘要 Abstract
探索对抗评价方法在开放域对话生成系统中的应用潜力与局限,比较判别器和人类在相同任务上的表现。
结果表明，任务对人类和判别器模型来说都很难，但判别器能够学习到模式并取得高于猜测 above-chance 的成绩

### 简介 Introduction
- 受图灵测试的启发：通过分析生成回复与人类回复的区分性，判断生成回复的质量

### 判别器 The Discriminative Agent
- 判别器是一个二分类模型（sigmoid 0-1），给定一个回复，判断它是‘真’（语料中的对话对）的，还是‘假’（最后一句话被随机替换）的
- 判别器是基于注意力机制的双向LSTM

### 训练细节 Training Details
- 数据集: MovieTriples, SubTle and Switchboard,最后一句作为目标回复，其余作为上下文


### 人类评价 Human Evaluation
- 任务：给定上下文，判断给出的回复是原本的回复还是随机的回复
- 每个数据集随机选取300个对话对，一半保留原始回复，一半替换为随机回复
- 每一个对话对由3名评分者进行评价，对话者之间的一致Fleiss's Kappa为0.3，说明这个任务对人类来说很有挑战性
- 人类只有在回复及其不合理时才将回复标记为随机回复，因而正确结果的召回率很高，随机结果的准确率很高

### 分析 Analysis
- 进一步分析判别器利用了什么信息进行预测
- 运行两次额外的前向遍历，分别输入上下文和回复，并计算各个LSTM隐藏状态之间的余弦相似度
- 发现上下文和回复的余弦相似度很高时更容易被判别器预测为连贯的对话
- 连续对话的上下文和回复的余弦相似度会更高
- 在SubTle中，判别器准确度高于人类，因此判别器可能对人类可能看不到的模式很敏感

### 结果 Results
- 使用OpenSubtitles，训练一个聊天机器人，判别器和人类判断回复是系统生成回复还是语料真实回复
- 任务2：对之前提到的三个数据集分别选取30个对话对，聊天机器人分别产生回复，对训练的聊天机器人的生成回复与参考回复进行判断
- 结果表明，人类表现是随机选择水平 chance-level， 判别器高于 chance-level，说明判别器能够看到人类看不到的一些模式

### 结论 Conclusion
任务是有难度的，当上下文只有一两句时，人类评价时的共识很少
