## A Survey of Availabile Corpora for Building Data-Driven Dialogue Systems ##
## 评价指标 Evaluation metrics ##
在对话系统中最有挑战的一个主题是对话系统的评价，其最终目标是将对话系统部署到实际应用场景，接收用户的真实反馈，然而这种方法是耗费时间和资源的。
目前有亚马逊众包平台可以雇佣众包人员对对话系统进行人工评价，但不同的众包人员的评价结果可能会有偏差。

此外在一个 伪绩效指标（pseudo-performance metric）上提升系统的表现是很有必要的，因为很多超参数的调整需要实验很多次。

因此，一般的评价过程为：先对不同的对话系统分别用自动评价指标进行评价得到一个分数，再选取得分较高的几个对话系统进行人工评价。

评价非任务型对话系统的起源为图灵测试。Schatzmann指出评价对话系统要关注两个方面：
1）模型是否可以生成类似人的输出 （human-like output）
2）模型是否可以重现语料库中的各种用户行为（user behaviour）

### 非目标驱动对话系统评估的自动评估指标 ###
- 词重叠指标 word overlap metrics  
可以借鉴其他NLP任务的评价指标如机器翻译中的BLEU和MENTOR。这些指标的评价方式为比较模型的输出与标准答案重叠的词语的多少。
所以使用BELEU和MENTOR至少需要一个标准答案。  
但有研究表明BLEU评价的结果与人类的评价是不相关的。因为在对话系统中给定一个输入，可以回答这个输入的回复有很多个。
而可能系统给出的回复虽然也是合理的，但与标准答案没有重叠的词语，这样BLEU分数就会很低，不能对模型的表现做出正确的评价。
即使MENTOR考虑了同义词和形态的变化，但也面临着上述问题。  
总之，BLEU和MENTOR只满足了Schatzmann的一个标准：高的BLEU和MENTOR分数只表明了模型能够产生类似人的输出，
但是该模型可能仍无法重现语料库中各种用户行为。

- 下一句话分类 Next Utterance Classification NUC  
将可能的回复的数量限制在一个小的列表里边,使训练好的模型在这个列表里选出最合适的回复。列表中包含一个正确回复,其他回复随机从语料库中抽取。
然后利用召回率和准确率指标对模型进行评价。  
NUC的优点是这种方法是可以解释的,而且任务的难度可以通过改变列表的大小实现。缺点是随机从语料库中抽取的负样本可能也是合适的回复。所以可以用
Recall@k(在前k个答案中是否有正确答案)指标进行评价。

- 困惑度 Word Perplexity  
困惑度衡量了模型在给定上下文的情况下生成标准的下一话语的概率大小。这对对话系统来说是有利的,因为有很多可能的回复。  
re-weighted perplexity忽略停用词,标点符号和句子结束符,只关注回复的语义内容。 
困惑度和NUC都满足的Schatzman的两个标准。

- 回复多样性 Response Diversity  
目前,非任务驱动的对话系统中面临着一个问题实现回复的多样性。为了评价模型产生回复的多样性,采用distinct-1,distinct-2对回复中不同的单词和词语的数量进行统计。 
虽然没有满足Schatzmann的两个标准,但是也是有用的评价指标。

目前,还是采用多种自动评价指标对系统进行评价比较可靠。但是具有与人类评价相关的自动评价指标是很重要的,
没有一个好的自动评价指标意味着无法对模型的好坏做出有效的判断,也就无法快速地推动对话领域的进步。
